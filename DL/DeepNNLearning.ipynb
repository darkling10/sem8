{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.00</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>396.90000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>396.90000</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>396.90000</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>393.45000</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>396.90000</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1012 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2    3      4   ...    6       7    8      9     10\n",
       "0       0.00632  18.00   2.31  0.0  0.538  ...  65.2  4.0900  1.0  296.0  15.3\n",
       "1     396.90000   4.98  24.00  NaN    NaN  ...   NaN     NaN  NaN    NaN   NaN\n",
       "2       0.02731   0.00   7.07  0.0  0.469  ...  78.9  4.9671  2.0  242.0  17.8\n",
       "3     396.90000   9.14  21.60  NaN    NaN  ...   NaN     NaN  NaN    NaN   NaN\n",
       "4       0.02729   0.00   7.07  0.0  0.469  ...  61.1  4.9671  2.0  242.0  17.8\n",
       "...         ...    ...    ...  ...    ...  ...   ...     ...  ...    ...   ...\n",
       "1007  396.90000   5.64  23.90  NaN    NaN  ...   NaN     NaN  NaN    NaN   NaN\n",
       "1008    0.10959   0.00  11.93  0.0  0.573  ...  89.3  2.3889  1.0  273.0  21.0\n",
       "1009  393.45000   6.48  22.00  NaN    NaN  ...   NaN     NaN  NaN    NaN   NaN\n",
       "1010    0.04741   0.00  11.93  0.0  0.573  ...  80.8  2.5050  1.0  273.0  21.0\n",
       "1011  396.90000   7.88  11.90  NaN    NaN  ...   NaN     NaN  NaN    NaN   NaN\n",
       "\n",
       "[1012 rows x 11 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = raw_df.values[1::2, 2]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu',input_shape=(X_train_scaled.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 3ms/step - loss: 565.9526 - mae: 21.8827\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 494.8617 - mae: 20.2161\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 409.6012 - mae: 18.0440\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 306.6339 - mae: 15.2068\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 192.9895 - mae: 11.6643\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 99.2516 - mae: 7.9885\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 54.8371 - mae: 5.6248\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 40.2057 - mae: 4.8007\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 32.3516 - mae: 4.2979\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 27.2072 - mae: 3.9313\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 24.3700 - mae: 3.7186\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 22.4495 - mae: 3.5632\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 21.3672 - mae: 3.4581\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 19.9933 - mae: 3.3285\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 19.1949 - mae: 3.2435\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 18.4072 - mae: 3.1666\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 17.8061 - mae: 3.0967\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 17.2837 - mae: 3.0481\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.6984 - mae: 2.9804\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.2937 - mae: 2.9313\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.9003 - mae: 2.9041\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.4742 - mae: 2.8722\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.1486 - mae: 2.8217\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 14.7620 - mae: 2.7823\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 14.5295 - mae: 2.7560\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 14.2410 - mae: 2.7341\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 14.0822 - mae: 2.6884\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 13.7080 - mae: 2.6749\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 13.4684 - mae: 2.6478\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 13.2091 - mae: 2.6072\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.9726 - mae: 2.5904\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.7427 - mae: 2.5552\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.5908 - mae: 2.5415\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.4137 - mae: 2.5143\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.2408 - mae: 2.4973\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.0350 - mae: 2.4725\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.9567 - mae: 2.4565\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.7732 - mae: 2.4412\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.6157 - mae: 2.4216\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.4900 - mae: 2.4153\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.3226 - mae: 2.3943\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.2205 - mae: 2.3793\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 11.0719 - mae: 2.3668\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.0457 - mae: 2.3673\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 10.8166 - mae: 2.3343\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 10.7141 - mae: 2.3246\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 10.7070 - mae: 2.3231\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.6114 - mae: 2.3116\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 10.4397 - mae: 2.3064\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.3256 - mae: 2.2820\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.2877 - mae: 2.2867\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.2213 - mae: 2.2747\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 10.0963 - mae: 2.2666\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.1399 - mae: 2.2778\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.9355 - mae: 2.2531\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.7834 - mae: 2.2316\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.6925 - mae: 2.2256\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.6225 - mae: 2.2075\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.6049 - mae: 2.2187\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.5416 - mae: 2.2071\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.3870 - mae: 2.1970\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.4850 - mae: 2.2195\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.2338 - mae: 2.1765\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.2141 - mae: 2.1748\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 9.1253 - mae: 2.1741\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.0734 - mae: 2.1457\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.1341 - mae: 2.1821\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.9201 - mae: 2.1555\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.8616 - mae: 2.1402\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.7529 - mae: 2.1324\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.7319 - mae: 2.1332\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.7218 - mae: 2.1369\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.7491 - mae: 2.1321\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.5437 - mae: 2.1113\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.4771 - mae: 2.1137\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.5270 - mae: 2.1167\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.3474 - mae: 2.0953\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.2861 - mae: 2.0876\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.3712 - mae: 2.1024\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.1734 - mae: 2.0736\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.0721 - mae: 2.0671\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.1666 - mae: 2.0845\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.0518 - mae: 2.0560\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 7.9009 - mae: 2.0465\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.0178 - mae: 2.0703\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.8913 - mae: 2.0435\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.7756 - mae: 2.0303\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.7235 - mae: 2.0209\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.6701 - mae: 2.0195\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.6430 - mae: 2.0130\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.5886 - mae: 2.0112\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.5383 - mae: 2.0093\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.5192 - mae: 1.9965\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.4545 - mae: 1.9962\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.4253 - mae: 1.9938\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.3449 - mae: 1.9822\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.2943 - mae: 1.9773\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.3786 - mae: 1.9795\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.2686 - mae: 1.9814\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.1674 - mae: 1.9528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27f87a02790>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled,y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2.2242841720581055\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_data(new_data):\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    predictions = model.predict(new_data_scaled)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('MPG')\n",
    "test_labels = test_features.pop('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted price: [[25.138956]]\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[0.00632, 18.0, 2.31, 0, 0.538, 6.575, 65.2, 4.09, 1, 296.0, 15.3, 396.9, 4.98]])\n",
    "print(\"Predicted price:\", predict_new_data(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
